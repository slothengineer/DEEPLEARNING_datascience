{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjQQhnv46nIv0BZrPSJUCe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slothengineer/DEEPLEARNING_datascience/blob/main/CNN_Architectures1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN- LENET"
      ],
      "metadata": {
        "id": "RWHSVKiB9TfX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKUG6BoA70vE"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_N274yn7-L_",
        "outputId": "4fb1bf47-86c8-4acc-a201-1b29c53fc227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 12s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize pixel values between 0 and 1\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ],
      "metadata": {
        "id": "GEVGncNZ8Gg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "Jek1ZpUE8Kat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(6, kernel_size = (5,5), padding = 'valid', activation='tanh', input_shape = (32,32,3)))\n",
        "model.add(AveragePooling2D(pool_size= (2,2), strides = 2, padding = 'valid'))\n",
        "\n",
        "model.add(Conv2D(16, kernel_size = (5,5), padding = 'valid', activation='tanh'))\n",
        "model.add(AveragePooling2D(pool_size= (2,2), strides = 2, padding = 'valid'))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(120, activation='tanh'))\n",
        "model.add(Dense(84, activation='tanh'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k56iGLIa84Au",
        "outputId": "50b28755-9431-47cc-b946-2cb1daa188ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 6)         456       \n",
            "                                                                 \n",
            " average_pooling2d (AverageP  (None, 14, 14, 6)        0         \n",
            " ooling2D)                                                       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " average_pooling2d_1 (Averag  (None, 5, 5, 16)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 400)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 120)               48120     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,006\n",
            "Trainable params: 62,006\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=2, verbose=1, validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test)\n",
        "\n",
        "print('Test Loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zYvOHK287xe",
        "outputId": "8e1df1f9-b20f-44a6-8ad2-e59e77a7d871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "391/391 [==============================] - 40s 99ms/step - loss: 1.8192 - accuracy: 0.3561 - val_loss: 1.7073 - val_accuracy: 0.3986\n",
            "Epoch 2/2\n",
            "391/391 [==============================] - 38s 98ms/step - loss: 1.6309 - accuracy: 0.4278 - val_loss: 1.5461 - val_accuracy: 0.4491\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 1.5461 - accuracy: 0.4491\n",
            "Test Loss: 1.5461255311965942\n",
            "Test accuracy: 0.44909998774528503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN- ALEXNET"
      ],
      "metadata": {
        "id": "IxcKtzPwGlMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tflearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-Pq04cPGrUO",
        "outputId": "2f4310ea-7b9d-4187-ed2b-2439a5e78ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/107.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m102.4/107.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn) (8.4.0)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127283 sha256=65cd81db09c457200ff92a67de7b2d695a06b50a7c5f9f9edbcbcab6ee373854\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/fb/7b/e06204a0ceefa45443930b9a250cb5ebe31def0e4e8245a465\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization"
      ],
      "metadata": {
        "id": "NeOF7n5kJXwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tflearn.datasets.oxflower17 as oxflower17\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "x, y = oxflower17.load_data()\n",
        "\n",
        "x_train = x.astype('float32') / 255.0\n",
        "y_train = to_categorical(y, num_classes=17)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWsEUH8uJe3H",
        "outputId": "d66b7cac-c5e1-4e01-93c9-7f7c9a36df1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Oxford 17 category Flower Dataset, Please wait...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100.0% 60276736 / 60270631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Succesfully downloaded 17flowers.tgz 60270631 bytes.\n",
            "File Extracted\n",
            "Starting to parse images...\n",
            "Parsing Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngILrpTNJiV_",
        "outputId": "7127d2e0-1c99-4b26-a3ff-aa0308e9968d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1360, 224, 224, 3)\n",
            "(1360, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation before passing it to the next layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "\n",
        "# 3rd Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 4th Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# 5th Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# Passing it to a dense layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# 1st Dense Layer\n",
        "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 2nd Dense Layer\n",
        "model.add(Dense(4096))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(17))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHrZEi18Jph8",
        "outputId": "4e562577-1b63-4bf7-c51f-20b01ac72f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/layers/normalization/batch_normalization.py:581: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 54, 54, 96)        34944     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 54, 54, 96)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 26, 26, 96)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 26, 26, 96)       384       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 26, 26, 256)       614656    \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 26, 26, 256)       0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 12, 12, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 12, 12, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 10, 10, 384)       885120    \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 10, 10, 384)       0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 10, 10, 384)      1536      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 384)         1327488   \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 8, 8, 384)         0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 8, 8, 384)        1536      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 6, 6, 256)         884992    \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 6, 6, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 2, 2, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 2, 2, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4096)              4198400   \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 4096)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4096)              0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 4096)             16384     \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 4096)              0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 4096)             16384     \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 17)                69649     \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 17)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,834,833\n",
            "Trainable params: 24,815,697\n",
            "Non-trainable params: 19,136\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "1Fu7lRYLKQEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=64, epochs=5, verbose=1,validation_split=0.2, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvTocBtOKZqQ",
        "outputId": "7ab76717-1c43-4997-b8c2-a154dc252020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 1088 samples, validate on 272 samples\n",
            "Epoch 1/5\n",
            "1088/1088 [==============================] - ETA: 0s - loss: 3.7124 - acc: 0.2335"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1088/1088 [==============================] - 139s 128ms/sample - loss: 3.7124 - acc: 0.2335 - val_loss: 3.3584 - val_acc: 0.0515\n",
            "Epoch 2/5\n",
            "1088/1088 [==============================] - 129s 118ms/sample - loss: 2.3127 - acc: 0.3980 - val_loss: 4.4682 - val_acc: 0.0515\n",
            "Epoch 3/5\n",
            "1088/1088 [==============================] - 128s 118ms/sample - loss: 1.9694 - acc: 0.4412 - val_loss: 6.4428 - val_acc: 0.0625\n",
            "Epoch 4/5\n",
            "1088/1088 [==============================] - 129s 119ms/sample - loss: 1.6552 - acc: 0.5221 - val_loss: 8.5114 - val_acc: 0.0625\n",
            "Epoch 5/5\n",
            "1088/1088 [==============================] - 128s 117ms/sample - loss: 1.5078 - acc: 0.5533 - val_loss: 5.5625 - val_acc: 0.0625\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x79bfc5c6ab30>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN- VGG"
      ],
      "metadata": {
        "id": "5DIcF5A8WyhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "HPXRhWrWKjs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tflearn.datasets.oxflower17 as oxflower17\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "x, y = oxflower17.load_data()\n",
        "\n",
        "x_train = x.astype('float32') / 255.0\n",
        "y_train = to_categorical(y, num_classes=17)"
      ],
      "metadata": {
        "id": "BG6Usw3TW6Ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a6f4e23-1897-43b5-841d-b7ce461e5410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbY6_dVvW8lq",
        "outputId": "1835efc8-7b27-4627-a018-b41c709ce6c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1360, 224, 224, 3)\n",
            "(1360, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=4096,activation=\"relu\"))\n",
        "model.add(Dense(units=4096,activation=\"relu\"))\n",
        "model.add(Dense(units=17, activation=\"softmax\"))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUdlpBauXBRZ",
        "outputId": "b19ca55c-8e64-4ea4-d0cb-35f94a148976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 112, 112, 64)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 56, 56, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 28, 28, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 14, 14, 512)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 7, 7, 512)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4096)              102764544 \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 17)                69649     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 134,330,193\n",
            "Trainable params: 134,330,193\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "jAUB3WiKXNYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=64, epochs=1, verbose=1,validation_split=0.2, shuffle=True)"
      ],
      "metadata": {
        "id": "YzjjjxsrXTFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5b3fde-ad33-4c24-c409-76d83c1e85bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 1088 samples, validate on 272 samples\n",
            "1088/1088 [==============================] - ETA: 0s - loss: 2.8381 - acc: 0.0542  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1088/1088 [==============================] - 2461s 2s/sample - loss: 2.8381 - acc: 0.0542 - val_loss: 2.8355 - val_acc: 0.0368\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x79eeeca2e620>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG- Pretrained"
      ],
      "metadata": {
        "id": "jk5UNi5sZP65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "url = \"https://drive.google.com/file/d/12jiQxJzYSYl3wnC8x5wHAhRzzJmmsCXP/view?usp=sharing\"\n",
        "file_id = url.split(\"/\")[-2]\n",
        "print(file_id)\n",
        "prefix = 'https://drive.google.com/uc?/export=download&id='\n",
        "gdown.download(prefix+file_id, \"catdog.zip\")"
      ],
      "metadata": {
        "id": "iFWhKn5PXWKF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "bc7f30ea-6ddb-4fcf-d10b-6a4b32c95a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12jiQxJzYSYl3wnC8x5wHAhRzzJmmsCXP\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?/export=download&id=12jiQxJzYSYl3wnC8x5wHAhRzzJmmsCXP\n",
            "To: /content/catdog.zip\n",
            "100%|██████████| 9.09M/9.09M [00:00<00:00, 11.4MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'catdog.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip catdog.zip"
      ],
      "metadata": {
        "id": "trsBmlxAZXPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c06e2f-b177-487f-cff3-8c7df54ddd5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  catdog.zip\n",
            "   creating: train/\n",
            "   creating: train/Cat/\n",
            "  inflating: train/Cat/0.jpg         \n",
            "  inflating: train/Cat/1.jpg         \n",
            "  inflating: train/Cat/2.jpg         \n",
            "  inflating: train/Cat/cat.2405.jpg  \n",
            "  inflating: train/Cat/cat.2406.jpg  \n",
            "  inflating: train/Cat/cat.2436.jpg  \n",
            "  inflating: train/Cat/cat.2437.jpg  \n",
            "  inflating: train/Cat/cat.2438.jpg  \n",
            "  inflating: train/Cat/cat.2439.jpg  \n",
            "  inflating: train/Cat/cat.2440.jpg  \n",
            "  inflating: train/Cat/cat.2441.jpg  \n",
            "  inflating: train/Cat/cat.2442.jpg  \n",
            "  inflating: train/Cat/cat.2443.jpg  \n",
            "  inflating: train/Cat/cat.2444.jpg  \n",
            "  inflating: train/Cat/cat.2445.jpg  \n",
            "  inflating: train/Cat/cat.2446.jpg  \n",
            "  inflating: train/Cat/cat.2447.jpg  \n",
            "  inflating: train/Cat/cat.2448.jpg  \n",
            "  inflating: train/Cat/cat.2449.jpg  \n",
            "  inflating: train/Cat/cat.2450.jpg  \n",
            "  inflating: train/Cat/cat.2451.jpg  \n",
            "  inflating: train/Cat/cat.2452.jpg  \n",
            "  inflating: train/Cat/cat.2453.jpg  \n",
            "  inflating: train/Cat/cat.2454.jpg  \n",
            "  inflating: train/Cat/cat.2455.jpg  \n",
            "  inflating: train/Cat/cat.2456.jpg  \n",
            "  inflating: train/Cat/cat.2457.jpg  \n",
            "  inflating: train/Cat/cat.2458.jpg  \n",
            "  inflating: train/Cat/cat.2459.jpg  \n",
            "  inflating: train/Cat/cat.2460.jpg  \n",
            "  inflating: train/Cat/cat.2461.jpg  \n",
            "  inflating: train/Cat/cat.2462.jpg  \n",
            "  inflating: train/Cat/cat.2463.jpg  \n",
            "  inflating: train/Cat/cat.2464.jpg  \n",
            "  inflating: train/Cat/cat.855.jpg   \n",
            "  inflating: train/Cat/cat.856.jpg   \n",
            "  inflating: train/Cat/cat.857.jpg   \n",
            "  inflating: train/Cat/cat.858.jpg   \n",
            "  inflating: train/Cat/cat.859.jpg   \n",
            "  inflating: train/Cat/cat.86.jpg    \n",
            "  inflating: train/Cat/cat.860.jpg   \n",
            "  inflating: train/Cat/cat.861.jpg   \n",
            "  inflating: train/Cat/cat.862.jpg   \n",
            "  inflating: train/Cat/cat.863.jpg   \n",
            "  inflating: train/Cat/cat.864.jpg   \n",
            "  inflating: train/Cat/cat.865.jpg   \n",
            "  inflating: train/Cat/cat.866.jpg   \n",
            "  inflating: train/Cat/cat.867.jpg   \n",
            "  inflating: train/Cat/cat.868.jpg   \n",
            "  inflating: train/Cat/cat.869.jpg   \n",
            "  inflating: train/Cat/cat.87.jpg    \n",
            "  inflating: train/Cat/cat.870.jpg   \n",
            "  inflating: train/Cat/cat.871.jpg   \n",
            "  inflating: train/Cat/cat.872.jpg   \n",
            "  inflating: train/Cat/cat.873.jpg   \n",
            "  inflating: train/Cat/cat.874.jpg   \n",
            "  inflating: train/Cat/cat.875.jpg   \n",
            "  inflating: train/Cat/cat.876.jpg   \n",
            "  inflating: train/Cat/cat.877.jpg   \n",
            "  inflating: train/Cat/cat.878.jpg   \n",
            "  inflating: train/Cat/cat.879.jpg   \n",
            "  inflating: train/Cat/cat.88.jpg    \n",
            "  inflating: train/Cat/cat.880.jpg   \n",
            "  inflating: train/Cat/cat.881.jpg   \n",
            "  inflating: train/Cat/cat.882.jpg   \n",
            "  inflating: train/Cat/cat.883.jpg   \n",
            "  inflating: train/Cat/cat.884.jpg   \n",
            "  inflating: train/Cat/cat.885.jpg   \n",
            "  inflating: train/Cat/cat.886.jpg   \n",
            "  inflating: train/Cat/cat.887.jpg   \n",
            "  inflating: train/Cat/cat.888.jpg   \n",
            "  inflating: train/Cat/cat.889.jpg   \n",
            "  inflating: train/Cat/cat.89.jpg    \n",
            "  inflating: train/Cat/cat.890.jpg   \n",
            "  inflating: train/Cat/cat.891.jpg   \n",
            "  inflating: train/Cat/cat.892.jpg   \n",
            "  inflating: train/Cat/cat.893.jpg   \n",
            "  inflating: train/Cat/cat.894.jpg   \n",
            "  inflating: train/Cat/cat.895.jpg   \n",
            "  inflating: train/Cat/cat.896.jpg   \n",
            "  inflating: train/Cat/cat.897.jpg   \n",
            "  inflating: train/Cat/cat.898.jpg   \n",
            "  inflating: train/Cat/cat.899.jpg   \n",
            "  inflating: train/Cat/cat.9.jpg     \n",
            "  inflating: train/Cat/cat.90.jpg    \n",
            "  inflating: train/Cat/cat.900.jpg   \n",
            "  inflating: train/Cat/cat.901.jpg   \n",
            "  inflating: train/Cat/cat.902.jpg   \n",
            "  inflating: train/Cat/cat.903.jpg   \n",
            "  inflating: train/Cat/cat.904.jpg   \n",
            "  inflating: train/Cat/cat.905.jpg   \n",
            "  inflating: train/Cat/cat.906.jpg   \n",
            "  inflating: train/Cat/cat.907.jpg   \n",
            "  inflating: train/Cat/cat.908.jpg   \n",
            "  inflating: train/Cat/cat.909.jpg   \n",
            "  inflating: train/Cat/cat.91.jpg    \n",
            "  inflating: train/Cat/cat.910.jpg   \n",
            "  inflating: train/Cat/cat.911.jpg   \n",
            "  inflating: train/Cat/cat.912.jpg   \n",
            "  inflating: train/Cat/cat.913.jpg   \n",
            "  inflating: train/Cat/cat.914.jpg   \n",
            "  inflating: train/Cat/cat.915.jpg   \n",
            "  inflating: train/Cat/cat.916.jpg   \n",
            "  inflating: train/Cat/cat.917.jpg   \n",
            "  inflating: train/Cat/cat.918.jpg   \n",
            "  inflating: train/Cat/cat.919.jpg   \n",
            "  inflating: train/Cat/cat.92.jpg    \n",
            "  inflating: train/Cat/cat.920.jpg   \n",
            "  inflating: train/Cat/cat.93.jpg    \n",
            "  inflating: train/Cat/cat.94.jpg    \n",
            "  inflating: train/Cat/cat.946.jpg   \n",
            "  inflating: train/Cat/cat.947.jpg   \n",
            "  inflating: train/Cat/cat.948.jpg   \n",
            "  inflating: train/Cat/cat.949.jpg   \n",
            "  inflating: train/Cat/cat.95.jpg    \n",
            "  inflating: train/Cat/cat.950.jpg   \n",
            "  inflating: train/Cat/cat.951.jpg   \n",
            "  inflating: train/Cat/cat.952.jpg   \n",
            "  inflating: train/Cat/cat.953.jpg   \n",
            "  inflating: train/Cat/cat.954.jpg   \n",
            "  inflating: train/Cat/cat.955.jpg   \n",
            "  inflating: train/Cat/cat.956.jpg   \n",
            "  inflating: train/Cat/cat.957.jpg   \n",
            "  inflating: train/Cat/cat.958.jpg   \n",
            "  inflating: train/Cat/cat.959.jpg   \n",
            "  inflating: train/Cat/cat.96.jpg    \n",
            "  inflating: train/Cat/cat.960.jpg   \n",
            "  inflating: train/Cat/cat.961.jpg   \n",
            "  inflating: train/Cat/cat.962.jpg   \n",
            "  inflating: train/Cat/cat.963.jpg   \n",
            "  inflating: train/Cat/cat.964.jpg   \n",
            "  inflating: train/Cat/cat.965.jpg   \n",
            "  inflating: train/Cat/cat.966.jpg   \n",
            "  inflating: train/Cat/cat.967.jpg   \n",
            "  inflating: train/Cat/cat.968.jpg   \n",
            "  inflating: train/Cat/cat.969.jpg   \n",
            "  inflating: train/Cat/cat.97.jpg    \n",
            "  inflating: train/Cat/cat.970.jpg   \n",
            "  inflating: train/Cat/cat.971.jpg   \n",
            "  inflating: train/Cat/cat.972.jpg   \n",
            "  inflating: train/Cat/cat.973.jpg   \n",
            "  inflating: train/Cat/cat.974.jpg   \n",
            "  inflating: train/Cat/cat.975.jpg   \n",
            "  inflating: train/Cat/cat.976.jpg   \n",
            "  inflating: train/Cat/cat.977.jpg   \n",
            "  inflating: train/Cat/cat.978.jpg   \n",
            "  inflating: train/Cat/cat.979.jpg   \n",
            "  inflating: train/Cat/cat.98.jpg    \n",
            "  inflating: train/Cat/cat.980.jpg   \n",
            "  inflating: train/Cat/cat.981.jpg   \n",
            "  inflating: train/Cat/cat.982.jpg   \n",
            "  inflating: train/Cat/cat.983.jpg   \n",
            "  inflating: train/Cat/cat.984.jpg   \n",
            "  inflating: train/Cat/cat.985.jpg   \n",
            "  inflating: train/Cat/cat.986.jpg   \n",
            "  inflating: train/Cat/cat.987.jpg   \n",
            "  inflating: train/Cat/cat.988.jpg   \n",
            "  inflating: train/Cat/cat.989.jpg   \n",
            "  inflating: train/Cat/cat.99.jpg    \n",
            "  inflating: train/Cat/cat.990.jpg   \n",
            "  inflating: train/Cat/cat.991.jpg   \n",
            "  inflating: train/Cat/cat.992.jpg   \n",
            "  inflating: train/Cat/cat.993.jpg   \n",
            "  inflating: train/Cat/cat.994.jpg   \n",
            "  inflating: train/Cat/cat.995.jpg   \n",
            "  inflating: train/Cat/cat.996.jpg   \n",
            "  inflating: train/Cat/cat.997.jpg   \n",
            "  inflating: train/Cat/cat.998.jpg   \n",
            "  inflating: train/Cat/cat.999.jpg   \n",
            "   creating: train/Dog/\n",
            "  inflating: train/Dog/10493.jpg     \n",
            "  inflating: train/Dog/11785.jpg     \n",
            "  inflating: train/Dog/9839.jpg      \n",
            "  inflating: train/Dog/dog.2432.jpg  \n",
            "  inflating: train/Dog/dog.2433.jpg  \n",
            "  inflating: train/Dog/dog.2434.jpg  \n",
            "  inflating: train/Dog/dog.2435.jpg  \n",
            "  inflating: train/Dog/dog.2436.jpg  \n",
            "  inflating: train/Dog/dog.2437.jpg  \n",
            "  inflating: train/Dog/dog.2438.jpg  \n",
            "  inflating: train/Dog/dog.2439.jpg  \n",
            "  inflating: train/Dog/dog.2440.jpg  \n",
            "  inflating: train/Dog/dog.2441.jpg  \n",
            "  inflating: train/Dog/dog.2442.jpg  \n",
            "  inflating: train/Dog/dog.2443.jpg  \n",
            "  inflating: train/Dog/dog.2444.jpg  \n",
            "  inflating: train/Dog/dog.2445.jpg  \n",
            "  inflating: train/Dog/dog.2446.jpg  \n",
            "  inflating: train/Dog/dog.2447.jpg  \n",
            "  inflating: train/Dog/dog.2448.jpg  \n",
            "  inflating: train/Dog/dog.2449.jpg  \n",
            "  inflating: train/Dog/dog.2450.jpg  \n",
            "  inflating: train/Dog/dog.2451.jpg  \n",
            "  inflating: train/Dog/dog.2452.jpg  \n",
            "  inflating: train/Dog/dog.2453.jpg  \n",
            "  inflating: train/Dog/dog.2454.jpg  \n",
            "  inflating: train/Dog/dog.2455.jpg  \n",
            "  inflating: train/Dog/dog.2456.jpg  \n",
            "  inflating: train/Dog/dog.2457.jpg  \n",
            "  inflating: train/Dog/dog.2458.jpg  \n",
            "  inflating: train/Dog/dog.2459.jpg  \n",
            "  inflating: train/Dog/dog.2460.jpg  \n",
            "  inflating: train/Dog/dog.2461.jpg  \n",
            "  inflating: train/Dog/dog.844.jpg   \n",
            "  inflating: train/Dog/dog.845.jpg   \n",
            "  inflating: train/Dog/dog.846.jpg   \n",
            "  inflating: train/Dog/dog.847.jpg   \n",
            "  inflating: train/Dog/dog.848.jpg   \n",
            "  inflating: train/Dog/dog.849.jpg   \n",
            "  inflating: train/Dog/dog.85.jpg    \n",
            "  inflating: train/Dog/dog.850.jpg   \n",
            "  inflating: train/Dog/dog.851.jpg   \n",
            "  inflating: train/Dog/dog.852.jpg   \n",
            "  inflating: train/Dog/dog.853.jpg   \n",
            "  inflating: train/Dog/dog.854.jpg   \n",
            "  inflating: train/Dog/dog.855.jpg   \n",
            "  inflating: train/Dog/dog.856.jpg   \n",
            "  inflating: train/Dog/dog.857.jpg   \n",
            "  inflating: train/Dog/dog.858.jpg   \n",
            "  inflating: train/Dog/dog.859.jpg   \n",
            "  inflating: train/Dog/dog.86.jpg    \n",
            "  inflating: train/Dog/dog.860.jpg   \n",
            "  inflating: train/Dog/dog.861.jpg   \n",
            "  inflating: train/Dog/dog.862.jpg   \n",
            "  inflating: train/Dog/dog.863.jpg   \n",
            "  inflating: train/Dog/dog.864.jpg   \n",
            "  inflating: train/Dog/dog.865.jpg   \n",
            "  inflating: train/Dog/dog.866.jpg   \n",
            "  inflating: train/Dog/dog.867.jpg   \n",
            "  inflating: train/Dog/dog.868.jpg   \n",
            "  inflating: train/Dog/dog.869.jpg   \n",
            "  inflating: train/Dog/dog.87.jpg    \n",
            "  inflating: train/Dog/dog.870.jpg   \n",
            "  inflating: train/Dog/dog.871.jpg   \n",
            "  inflating: train/Dog/dog.872.jpg   \n",
            "  inflating: train/Dog/dog.873.jpg   \n",
            "  inflating: train/Dog/dog.874.jpg   \n",
            "  inflating: train/Dog/dog.875.jpg   \n",
            "  inflating: train/Dog/dog.876.jpg   \n",
            "  inflating: train/Dog/dog.877.jpg   \n",
            "  inflating: train/Dog/dog.878.jpg   \n",
            "  inflating: train/Dog/dog.879.jpg   \n",
            "  inflating: train/Dog/dog.88.jpg    \n",
            "  inflating: train/Dog/dog.880.jpg   \n",
            "  inflating: train/Dog/dog.881.jpg   \n",
            "  inflating: train/Dog/dog.882.jpg   \n",
            "  inflating: train/Dog/dog.883.jpg   \n",
            "  inflating: train/Dog/dog.884.jpg   \n",
            "  inflating: train/Dog/dog.885.jpg   \n",
            "  inflating: train/Dog/dog.886.jpg   \n",
            "  inflating: train/Dog/dog.887.jpg   \n",
            "  inflating: train/Dog/dog.888.jpg   \n",
            "  inflating: train/Dog/dog.889.jpg   \n",
            "  inflating: train/Dog/dog.89.jpg    \n",
            "  inflating: train/Dog/dog.890.jpg   \n",
            "  inflating: train/Dog/dog.891.jpg   \n",
            "  inflating: train/Dog/dog.892.jpg   \n",
            "  inflating: train/Dog/dog.893.jpg   \n",
            "  inflating: train/Dog/dog.894.jpg   \n",
            "  inflating: train/Dog/dog.895.jpg   \n",
            "  inflating: train/Dog/dog.896.jpg   \n",
            "  inflating: train/Dog/dog.897.jpg   \n",
            "  inflating: train/Dog/dog.898.jpg   \n",
            "  inflating: train/Dog/dog.9.jpg     \n",
            "  inflating: train/Dog/dog.90.jpg    \n",
            "  inflating: train/Dog/dog.91.jpg    \n",
            "  inflating: train/Dog/dog.92.jpg    \n",
            "  inflating: train/Dog/dog.93.jpg    \n",
            "  inflating: train/Dog/dog.936.jpg   \n",
            "  inflating: train/Dog/dog.937.jpg   \n",
            "  inflating: train/Dog/dog.938.jpg   \n",
            "  inflating: train/Dog/dog.939.jpg   \n",
            "  inflating: train/Dog/dog.94.jpg    \n",
            "  inflating: train/Dog/dog.940.jpg   \n",
            "  inflating: train/Dog/dog.941.jpg   \n",
            "  inflating: train/Dog/dog.942.jpg   \n",
            "  inflating: train/Dog/dog.943.jpg   \n",
            "  inflating: train/Dog/dog.944.jpg   \n",
            "  inflating: train/Dog/dog.945.jpg   \n",
            "  inflating: train/Dog/dog.946.jpg   \n",
            "  inflating: train/Dog/dog.947.jpg   \n",
            "  inflating: train/Dog/dog.948.jpg   \n",
            "  inflating: train/Dog/dog.949.jpg   \n",
            "  inflating: train/Dog/dog.95.jpg    \n",
            "  inflating: train/Dog/dog.950.jpg   \n",
            "  inflating: train/Dog/dog.951.jpg   \n",
            "  inflating: train/Dog/dog.952.jpg   \n",
            "  inflating: train/Dog/dog.953.jpg   \n",
            "  inflating: train/Dog/dog.954.jpg   \n",
            "  inflating: train/Dog/dog.955.jpg   \n",
            "  inflating: train/Dog/dog.956.jpg   \n",
            "  inflating: train/Dog/dog.957.jpg   \n",
            "  inflating: train/Dog/dog.958.jpg   \n",
            "  inflating: train/Dog/dog.959.jpg   \n",
            "  inflating: train/Dog/dog.96.jpg    \n",
            "  inflating: train/Dog/dog.960.jpg   \n",
            "  inflating: train/Dog/dog.961.jpg   \n",
            "  inflating: train/Dog/dog.962.jpg   \n",
            "  inflating: train/Dog/dog.963.jpg   \n",
            "  inflating: train/Dog/dog.964.jpg   \n",
            "  inflating: train/Dog/dog.965.jpg   \n",
            "  inflating: train/Dog/dog.966.jpg   \n",
            "  inflating: train/Dog/dog.967.jpg   \n",
            "  inflating: train/Dog/dog.968.jpg   \n",
            "  inflating: train/Dog/dog.969.jpg   \n",
            "  inflating: train/Dog/dog.97.jpg    \n",
            "  inflating: train/Dog/dog.970.jpg   \n",
            "  inflating: train/Dog/dog.971.jpg   \n",
            "  inflating: train/Dog/dog.972.jpg   \n",
            "  inflating: train/Dog/dog.973.jpg   \n",
            "  inflating: train/Dog/dog.974.jpg   \n",
            "  inflating: train/Dog/dog.975.jpg   \n",
            "  inflating: train/Dog/dog.976.jpg   \n",
            "  inflating: train/Dog/dog.977.jpg   \n",
            "  inflating: train/Dog/dog.978.jpg   \n",
            "  inflating: train/Dog/dog.979.jpg   \n",
            "  inflating: train/Dog/dog.98.jpg    \n",
            "  inflating: train/Dog/dog.980.jpg   \n",
            "  inflating: train/Dog/dog.981.jpg   \n",
            "  inflating: train/Dog/dog.982.jpg   \n",
            "  inflating: train/Dog/dog.983.jpg   \n",
            "  inflating: train/Dog/dog.984.jpg   \n",
            "  inflating: train/Dog/dog.985.jpg   \n",
            "  inflating: train/Dog/dog.986.jpg   \n",
            "  inflating: train/Dog/dog.987.jpg   \n",
            "  inflating: train/Dog/dog.988.jpg   \n",
            "  inflating: train/Dog/dog.989.jpg   \n",
            "  inflating: train/Dog/dog.99.jpg    \n",
            "  inflating: train/Dog/dog.990.jpg   \n",
            "  inflating: train/Dog/dog.991.jpg   \n",
            "  inflating: train/Dog/dog.992.jpg   \n",
            "  inflating: train/Dog/dog.993.jpg   \n",
            "  inflating: train/Dog/dog.994.jpg   \n",
            "  inflating: train/Dog/dog.995.jpg   \n",
            "  inflating: train/Dog/dog.996.jpg   \n",
            "  inflating: train/Dog/dog.997.jpg   \n",
            "  inflating: train/Dog/dog.998.jpg   \n",
            "  inflating: train/Dog/dog.999.jpg   \n",
            "   creating: validation/\n",
            "   creating: validation/Cat/\n",
            "  inflating: validation/Cat/cat.2407.jpg  \n",
            "  inflating: validation/Cat/cat.2408.jpg  \n",
            "  inflating: validation/Cat/cat.2409.jpg  \n",
            "  inflating: validation/Cat/cat.2410.jpg  \n",
            "  inflating: validation/Cat/cat.2411.jpg  \n",
            "  inflating: validation/Cat/cat.2412.jpg  \n",
            "  inflating: validation/Cat/cat.2413.jpg  \n",
            "  inflating: validation/Cat/cat.2414.jpg  \n",
            "  inflating: validation/Cat/cat.2415.jpg  \n",
            "  inflating: validation/Cat/cat.2416.jpg  \n",
            "  inflating: validation/Cat/cat.2417.jpg  \n",
            "  inflating: validation/Cat/cat.2418.jpg  \n",
            "  inflating: validation/Cat/cat.2419.jpg  \n",
            "  inflating: validation/Cat/cat.2420.jpg  \n",
            "  inflating: validation/Cat/cat.2421.jpg  \n",
            "  inflating: validation/Cat/cat.2422.jpg  \n",
            "  inflating: validation/Cat/cat.2423.jpg  \n",
            "  inflating: validation/Cat/cat.2424.jpg  \n",
            "  inflating: validation/Cat/cat.2425.jpg  \n",
            "  inflating: validation/Cat/cat.2426.jpg  \n",
            "  inflating: validation/Cat/cat.2427.jpg  \n",
            "  inflating: validation/Cat/cat.2428.jpg  \n",
            "  inflating: validation/Cat/cat.2429.jpg  \n",
            "  inflating: validation/Cat/cat.2430.jpg  \n",
            "  inflating: validation/Cat/cat.2431.jpg  \n",
            "  inflating: validation/Cat/cat.2432.jpg  \n",
            "  inflating: validation/Cat/cat.2433.jpg  \n",
            "  inflating: validation/Cat/cat.2434.jpg  \n",
            "  inflating: validation/Cat/cat.2435.jpg  \n",
            "   creating: validation/Dog/\n",
            "  inflating: validation/Dog/dog.2402.jpg  \n",
            "  inflating: validation/Dog/dog.2403.jpg  \n",
            "  inflating: validation/Dog/dog.2404.jpg  \n",
            "  inflating: validation/Dog/dog.2405.jpg  \n",
            "  inflating: validation/Dog/dog.2406.jpg  \n",
            "  inflating: validation/Dog/dog.2407.jpg  \n",
            "  inflating: validation/Dog/dog.2408.jpg  \n",
            "  inflating: validation/Dog/dog.2409.jpg  \n",
            "  inflating: validation/Dog/dog.2410.jpg  \n",
            "  inflating: validation/Dog/dog.2411.jpg  \n",
            "  inflating: validation/Dog/dog.2412.jpg  \n",
            "  inflating: validation/Dog/dog.2413.jpg  \n",
            "  inflating: validation/Dog/dog.2414.jpg  \n",
            "  inflating: validation/Dog/dog.2415.jpg  \n",
            "  inflating: validation/Dog/dog.2416.jpg  \n",
            "  inflating: validation/Dog/dog.2417.jpg  \n",
            "  inflating: validation/Dog/dog.2418.jpg  \n",
            "  inflating: validation/Dog/dog.2419.jpg  \n",
            "  inflating: validation/Dog/dog.2420.jpg  \n",
            "  inflating: validation/Dog/dog.2421.jpg  \n",
            "  inflating: validation/Dog/dog.2422.jpg  \n",
            "  inflating: validation/Dog/dog.2423.jpg  \n",
            "  inflating: validation/Dog/dog.2424.jpg  \n",
            "  inflating: validation/Dog/dog.2425.jpg  \n",
            "  inflating: validation/Dog/dog.2426.jpg  \n",
            "  inflating: validation/Dog/dog.2427.jpg  \n",
            "  inflating: validation/Dog/dog.2428.jpg  \n",
            "  inflating: validation/Dog/dog.2429.jpg  \n",
            "  inflating: validation/Dog/dog.2430.jpg  \n",
            "  inflating: validation/Dog/dog.2431.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "\n",
        "# Set the path to your training and validation data\n",
        "train_data_dir = '/content/train'\n",
        "validation_data_dir = '/content/validation'\n",
        "\n",
        "# Set the number of training and validation samples\n",
        "num_train_samples = 2000\n",
        "num_validation_samples = 800\n",
        "\n",
        "# Set the number of epochs and batch size\n",
        "epochs = 1\n",
        "batch_size = 16\n",
        "\n",
        "# Load the VGG16 model without the top layer\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the base model as a layer\n",
        "model.add(base_model)\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Preprocess the training and validation data\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=num_train_samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=num_validation_samples // batch_size)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('dog_cat_classifier.h5')"
      ],
      "metadata": {
        "id": "KL9cSPswZjEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c7a143-c8c1-4528-9d76-4d174a894b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 337 images belonging to 2 classes.\n",
            "Found 59 images belonging to 2 classes.\n",
            " 22/125 [====>.........................] - ETA: 16:37 - loss: 7.6635 - accuracy: 0.8635"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 125 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r125/125 [==============================] - 259s 2s/step - loss: 7.6635 - accuracy: 0.8635 - val_loss: 2.7759 - val_accuracy: 0.9661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN- RESNET"
      ],
      "metadata": {
        "id": "WveFqwPhG4RS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, BatchNormalization, Activation, Add, AveragePooling2D, Flatten, Dense\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "bfZqVPq0jEaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tflearn.datasets.oxflower17 as oxflower17\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "x, y = oxflower17.load_data()\n",
        "\n",
        "x_train = x.astype('float32') / 255.0\n",
        "y_train = to_categorical(y, num_classes=17)"
      ],
      "metadata": {
        "id": "OSqFlNTEHmO0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9253785-4a0f-47ce-a373-6e877a8de994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "m85XPTnHHscQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2049c3-68fa-4ba2-8b01-8f9b4eb866ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1360, 224, 224, 3)\n",
            "(1360, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual block\n",
        "def residual_block(x, filters, downsample=False):\n",
        "    strides = (2, 2) if downsample else (1, 1)\n",
        "\n",
        "    # First convolutional layer of the block\n",
        "    y = Conv2D(filters, kernel_size=(3, 3), strides=strides, padding='same')(x)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "\n",
        "    # Second convolutional layer of the block\n",
        "    y = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "\n",
        "    # Skip connection if downsample or number of filters change\n",
        "    if downsample:\n",
        "        x = Conv2D(filters, kernel_size=(1, 1), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "    # Add skip connection\n",
        "    y = Add()([x, y])\n",
        "    y = Activation('relu')(y)\n",
        "    return y\n",
        "\n",
        "# Build the ResNet model\n",
        "def resnet(input_shape, num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "    # Initial convolutional layer\n",
        "    x = Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Residual blocks\n",
        "    x = residual_block(x, filters=16)\n",
        "    x = residual_block(x, filters=16)\n",
        "    x = residual_block(x, filters=32, downsample=True)\n",
        "    x = residual_block(x, filters=32)\n",
        "    x = residual_block(x, filters=64, downsample=True)\n",
        "    x = residual_block(x, filters=64)\n",
        "\n",
        "    # Average pooling and output layer\n",
        "    x = AveragePooling2D(pool_size=(8, 8))(x)\n",
        "    x = Flatten()(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "kCEgmASDIE1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the ResNet model\n",
        "model = resnet(input_shape=(224, 224, 3), num_classes=17)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=5, verbose=1,validation_split=0.2, shuffle=True)"
      ],
      "metadata": {
        "id": "OMfz1mufIF8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98fd75ff-23c7-41e3-a58d-fef7531a2b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/layers/normalization/batch_normalization.py:581: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 1088 samples, validate on 272 samples\n",
            "Epoch 1/5\n",
            "1088/1088 [==============================] - ETA: 0s - loss: 2.7739 - acc: 0.1921 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1088/1088 [==============================] - 612s 563ms/sample - loss: 2.7739 - acc: 0.1921 - val_loss: 2.8294 - val_acc: 0.0772\n",
            "Epoch 2/5\n",
            "1088/1088 [==============================] - 601s 553ms/sample - loss: 1.6613 - acc: 0.4596 - val_loss: 2.9281 - val_acc: 0.0772\n",
            "Epoch 3/5\n",
            "1088/1088 [==============================] - 605s 556ms/sample - loss: 1.1633 - acc: 0.6213 - val_loss: 3.2661 - val_acc: 0.0772\n",
            "Epoch 4/5\n",
            "1088/1088 [==============================] - 595s 547ms/sample - loss: 0.9390 - acc: 0.7031 - val_loss: 3.7615 - val_acc: 0.0772\n",
            "Epoch 5/5\n",
            "1088/1088 [==============================] - 595s 547ms/sample - loss: 0.7140 - acc: 0.7730 - val_loss: 4.5287 - val_acc: 0.0772\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7b58e7b0e710>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}